{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of BrandonClassifierFINAL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxAnyPs5Uils"
      },
      "source": [
        "# Binary Classifier for Bio-Imaging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpUB2vOTU0bN"
      },
      "source": [
        "### Loading and Splitting Data for Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuQNWh3sVRNu"
      },
      "source": [
        "# Import Packages\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import statistics\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import io\n",
        "from PIL import Image "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJGdvxI5_4Oe"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIIrXaJqWmKp"
      },
      "source": [
        "# List of all conditions paths: tuple of (path to csv with intensities and img paths, directory with images)\n",
        "all_conditions = [(\"/content/gdrive/Shareddrives/Senior_Design_Team_2020-2021/Senior_Design_2020-2021/Training_Data/Final Training Data/Cells/Filtered/filtered_tables/jasp_NCR_table.csv\", \"/content/gdrive/Shareddrives/Senior_Design_Team_2020-2021/Senior_Design_2020-2021/Training_Data/Final Training Data/Cells/Filtered/jasp_cells_norm/\"), \n",
        "                  (\"/content/gdrive/Shareddrives/Senior_Design_Team_2020-2021/Senior_Design_2020-2021/Training_Data/Final Training Data/Cells/Filtered/filtered_tables/latb_NCR_table.csv\", \"/content/gdrive/Shareddrives/Senior_Design_Team_2020-2021/Senior_Design_2020-2021/Training_Data/Final Training Data/Cells/Filtered/latb_cells_norm/\"), \n",
        "                  (\"/content/gdrive/Shareddrives/Senior_Design_Team_2020-2021/Senior_Design_2020-2021/Training_Data/Final Training Data/Cells/Filtered/filtered_tables/sfm_NCR_table.csv\", \"/content/gdrive/Shareddrives/Senior_Design_Team_2020-2021/Senior_Design_2020-2021/Training_Data/Final Training Data/Cells/Filtered/sfm_cells_norm/\"), \n",
        "                  (\"/content/gdrive/Shareddrives/Senior_Design_Team_2020-2021/Senior_Design_2020-2021/Training_Data/Final Training Data/Cells/Filtered/filtered_tables/vc_NCR_table.csv\", \"/content/gdrive/Shareddrives/Senior_Design_Team_2020-2021/Senior_Design_2020-2021/Training_Data/Final Training Data/Cells/Filtered/vc_cells_norm/\"),\n",
        "                  (\"/content/gdrive/Shareddrives/Senior_Design_Team_2020-2021/Senior_Design_2020-2021/Training_Data/Final Training Data/Cells/Filtered/filtered_tables/serum_NCR_table.csv\", \"/content/gdrive/Shareddrives/Senior_Design_Team_2020-2021/Senior_Design_2020-2021/Training_Data/Final Training Data/Cells/Filtered/serum_cells_norm/\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYsTX6_4W_T0"
      },
      "source": [
        "# Plot histogram of Nuclear to Cytoplasmic ratios \n",
        "for condition in all_conditions:\n",
        "  df = pd.read_csv(condition[0])\n",
        "  \n",
        "  ncrs = []\n",
        "  for index, row in df.iterrows():\n",
        "    ratio = 0\n",
        "    if (row['Cytoplasmic mean intensitiy'] != 0 and row['Nuclear mean intensity'] != 0):\n",
        "      ratio = row['Nuclear mean intensity'] / row['Cytoplasmic mean intensitiy']\n",
        "      ncrs.append(ratio)\n",
        "\n",
        "  new_df = pd.DataFrame(ncrs, columns = ['ncr'])\n",
        "  new_df['ncr'].hist(alpha=0.5, label = condition[1].split('/')[-2], range = (1,3))\n",
        "  print (condition[1].split('/')[-2], np.nanmedian(ncrs))\n",
        "\n",
        "plt.xlabel('Nuclear to Cytoplasmic Ratio')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5eyvG64XDe9"
      },
      "source": [
        "# Split data (only actin channels) into respective directories \n",
        "\n",
        "for condition in all_conditions:\n",
        "\n",
        "    # Path to NCR datatable and create dataframe\n",
        "    df = pd.read_csv(condition[0])\n",
        "\n",
        "    # Change paths of all images to google drive path \n",
        "    gd_path = condition[1]\n",
        "        \n",
        "    # Create lists to hold paths of two categories \n",
        "    above_list = []\n",
        "    below_list = []\n",
        "\n",
        "    # Set thresholds \n",
        "    t_one = (3.5,5.5)\n",
        "    t_two = (9,40)\n",
        "\n",
        "    # Calculate ratio for each row and add to designated dataframe\n",
        "    for index, row in df.iterrows():\n",
        "      ratio = 0\n",
        "      if (row['Cytoplasmic mean intensitiy'] != 0 and row['Nuclear mean intensity'] != 0):\n",
        "          ratio = row['Nuclear mean intensity'] / row['Cytoplasmic mean intensitiy']\n",
        "          tail = os.path.split(row['File Path'])[1]\n",
        "          \n",
        "      if ratio >= t_two[0] and ratio <= t_two[1]:\n",
        "          above_list.append((tail, ratio))\n",
        "      elif ratio >= t_one[0] and ratio <= t_one[1]:\n",
        "          below_list.append((tail, ratio))\n",
        "\n",
        "    # Create resulting directory and move files to correct directories \n",
        "    train_dir = os.path.split(condition[0])[0] + '/training'\n",
        "    validation_dir = os.path.split(condition[0])[0] + '/validation'\n",
        "\n",
        "    high_path_training = os.path.split(condition[0])[0] + '/training/high_dir'\n",
        "    high_path_val = os.path.split(condition[0])[0] + '/validation/high_dir'\n",
        "    high_path_testing = os.path.split(condition[0])[0] + '/testing/high_dir'\n",
        "\n",
        "    low_path_training = os.path.split(condition[0])[0] + '/training/low_dir'\n",
        "    low_path_val = os.path.split(condition[0])[0] + '/validation/low_dir'\n",
        "    low_path_testing = os.path.split(condition[0])[0] + '/testing/low_dir'\n",
        "\n",
        "    for path in [high_path_training, high_path_val, high_path_testing, low_path_training, low_path_val, low_path_testing]:\n",
        "        if os.path.isdir(path) == False:\n",
        "            os.makedirs(path)\n",
        "\n",
        "    # Get partitions for training, validation, testing (70%, 20%, 10% respectively)\n",
        "    above_count = len(above_list)\n",
        "    high_train = [0, round(above_count * 0.7)]\n",
        "    high_val = [high_train[1], round(high_train[1] + (above_count * 0.2))]\n",
        "    high_test = [high_val[1], above_count]\n",
        "    high_ranges = [(high_train, high_path_training), (high_val, high_path_val), (high_test, high_path_testing)]\n",
        "\n",
        "    below_count = len(below_list)\n",
        "    low_train = [0, round(below_count * 0.7)]\n",
        "    low_val = [low_train[1], round(low_train[1] + (below_count * 0.2))]\n",
        "    low_test = [low_val[1], below_count]\n",
        "    low_ranges = [(low_train, low_path_training), (low_val, low_path_val), (low_test, low_path_testing)]\n",
        "\n",
        "    for each in high_ranges:\n",
        "        rang = each[0]\n",
        "        temp = above_list[rang[0]:rang[1]]\n",
        "        for img in temp:\n",
        "            new_path = each[1] + '/' + img[0]\n",
        "            image = Image.open(gd_path + img[0])\n",
        "            image.seek(1)\n",
        "            image.save(new_path)\n",
        "\n",
        "    for each in low_ranges:\n",
        "        rang = each[0]\n",
        "        temp = below_list[rang[0]:rang[1]]\n",
        "        for img in temp:\n",
        "            new_path = each[1] + '/' + img[0]\n",
        "            image = Image.open(gd_path + img[0])\n",
        "            image.seek(1)\n",
        "            image.save(new_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pFVznpFWKfM"
      },
      "source": [
        "# See sizes of each dataset\n",
        "print('Low Training: ' + str(len(os.listdir(os.path.join(low_path_training)))))\n",
        "print('High Training: ' + str(len(os.listdir(os.path.join(high_path_training)))))\n",
        "print('Low Validation: ' + str(len(os.listdir(os.path.join(low_path_val)))))\n",
        "print('High Validation: ' + str(len(os.listdir(os.path.join(high_path_val)))))\n",
        "print('Low Test: ' + str(len(os.listdir(os.path.join(low_path_testing)))))\n",
        "print('High Test: ' + str(len(os.listdir(os.path.join(high_path_testing)))))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN16Z_UBXrCQ"
      },
      "source": [
        "### Model and Train Binary Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnlmATlr8lol"
      },
      "source": [
        "# Import Packages\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing import image\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkNcIM-K-5D2"
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KMsYwftzkWe"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pc0a6nUcz9Jy"
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2F2C-V6vn-W"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function = tf.keras.applications.vgg16.preprocess_input)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function = tf.keras.applications.vgg16.preprocess_input)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory\n",
        "        train_dir,\n",
        "        # All images will be resized to 150x150\n",
        "        target_size=(150, 150),\n",
        "        batch_size=50,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=30,\n",
        "        class_mode='binary',\n",
        "        shuffle = False) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiwv8TVCwT3H"
      },
      "source": [
        "for data_batch, labels_batch in train_generator:\n",
        "    print('data batch shape:', data_batch.shape)\n",
        "    print('labels batch shape:', labels_batch.shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOyVBEDfwf-x"
      },
      "source": [
        "#Be sure to set batch size equal or near (num of images) / epochs\n",
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=28,  \n",
        "      epochs=14,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=13)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F1WNgmYv-0p"
      },
      "source": [
        "model.save('results_1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSwDCUSaablx"
      },
      "source": [
        "### Get Metrics "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdzCGBunv-3z"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9pnm0pZv-6x"
      },
      "source": [
        "y_pred = model.predict(validation_generator)\n",
        "y_true = validation_generator.classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTvDYtvQF3aS"
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_true,y_pred)\n",
        "\n",
        "auc = auc(fpr, tpr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUFK9msPF3kv"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr, tpr, label='Keras (area = {:.3f})'.format(auc))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvDCFsX7F3ux"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "num_of_val_samples = 1085\n",
        "batch_size = 20\n",
        "\n",
        "Y_pred = model.predict(validation_generator, num_of_val_samples //batch_size + 1)\n",
        "y_pred = np.argmax(Y_pred, axis = 1)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(validation_generator.classes, y_pred))\n",
        "print('Classification Report')\n",
        "target_names = ['low_validation', 'high_validation']\n",
        "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSqh858Dx35N"
      },
      "source": [
        "### With Data Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EILQhrXv-9B"
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "      rotation_range=50,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1FYHYzbv_Bl"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS8y4sy1F3ZI"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUlzNKkyv_EK"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=50,\n",
        "        class_mode='binary',\n",
        "        shuffle = False)\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=35,\n",
        "        class_mode='binary')\n",
        "\n",
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=68,\n",
        "      epochs=12,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=24)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubcDPx8ev_Ge"
      },
      "source": [
        "model.save('result_2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0en1roryrvo"
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDSViqij0pMC"
      },
      "source": [
        "y_pred = model.predict(validation_generator)\n",
        "y_true = validation_generator.classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMr_2maIzt65"
      },
      "source": [
        "con_mat = tf.math.confusion_matrix(y_true, y_pred).numpy()\n",
        "tn, fp, fn, tp = con_mat.ravel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh3CPBnBO33T"
      },
      "source": [
        "con_mat_norm = np.around(con_mat.astype('float')/\n",
        "                         con_mat.sum(axis =1)[:,np.newaxis],\n",
        "                         decimals = 1)\n",
        "classes = (0,1) \n",
        "con_mat_df = pd.DataFrame(con_mat_norm,\n",
        "                     index = classes, \n",
        "                     columns = classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsOLjs67WI5R"
      },
      "source": [
        "test_dir = '/content/drive/Shareddrives/Senior_Design_Team_2020-2021/Senior_Design_2020-2021/Training_Data/Final Training Data/Cells/ClassifierData/testing'\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=None,\n",
        "        class_mode='binary')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3dM5GeFEbLv"
      },
      "source": [
        "tst_acc = test.history['acc']\n",
        "test_acc = test.history['val_acc']\n",
        "tst_loss = test.history['loss']\n",
        "test_loss = test.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}